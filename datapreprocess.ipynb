{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv(\"all1819.csv\")\n",
    "\n",
    "ban = dataset[dataset['class'] ==\"['বাংলাদেশ']\"]\n",
    "sports = dataset[dataset['class'] ==\"['খেলা']\"]\n",
    "interN = dataset[dataset['class'] ==\"['আন্তর্জাতিক']\"]\n",
    "dur = dataset[dataset['class'] ==\"['দূর পরবাস']\"]\n",
    "enet = dataset[dataset['class'] ==\"['বিনোদন']\"]\n",
    "other = dataset[dataset['class'] ==\"['']\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['বাংলাদেশ']       6348\n",
       "['খেলা']           2247\n",
       "['আন্তর্জাতিক']    1456\n",
       "['দূর পরবাস']       853\n",
       "['বিনোদন']          789\n",
       "['']                487\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(dataset.index[dataset['class'] ==\"['অর্থনীতি']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['মতামত']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['জীবনযাপন']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['বিজ্ঞান ও প্রযুক্তি']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['শিক্ষা']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['আমরা']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['পাঁচমিশালি']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['বিশেষ সংখ্যা']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['তারুণ্যের জয়োৎসব']\"],inplace = True)\n",
    "dataset.drop(dataset.index[dataset['class'] ==\"['ফিচাররস+আলো']\"],inplace = True)\n",
    "\n",
    "dataset['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#puntuation remove\n",
    "def puncRem(words):\n",
    "    pnList = ['.', ',', ':', '(', ')', '[', ']', '{', '}', '`', '*', '&', '‘', '’', '–']\n",
    "    filtPunc = [w for w in words if w not in pnList]\n",
    "\n",
    "    return  filtPunc\n",
    "\n",
    "\n",
    "#remove stopword\n",
    "def remStWords(punctuF):\n",
    "    stWord = open(\"stopWord.txt\", encoding='utf8')\n",
    "    stWordFile = [i.rstrip() for i in stWord]\n",
    "    filtStWord = [w for w in punctuF if w not in stWordFile]\n",
    "    \n",
    "    return filtStWord\n",
    "\n",
    "def scoreWord(text):\n",
    "    wordScDict={}\n",
    "    for word in text:\n",
    "        count=1\n",
    "        if word in wordScDict:\n",
    "            count+=wordScDict.get(word)\n",
    "        temp={word:count}\n",
    "        wordScDict.update(temp)\n",
    "    return wordScDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Doc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-f7dbb29333af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Document\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Doc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" ইহাতে মন্তব্য প্রদান বন্ধ রয়েছে                                |\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(\"ইহাতে মন্তব্য প্রদান বন্ধ রয়েছে\" in texts[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Doc'"
     ]
    }
   ],
   "source": [
    "texts = dataset[\"Document\"]\n",
    "for i in range(len(texts)):\n",
    "    dataset[\"Doc\"][i]=texts[i].strip(\" ইহাতে মন্তব্য প্রদান বন্ধ রয়েছে                                |\")\n",
    "    #print(\"ইহাতে মন্তব্য প্রদান বন্ধ রয়েছে\" in texts[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "print(dataset[\"Doc\"][13300])\n",
    "\n",
    "dataset[\"Doc\"].to_csv(\"Pred.csv\",encoding='utf-8-sig',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['বাংলাদেশ']               518\n",
       "['খেলা']                   126\n",
       "['আন্তর্জাতিক']            118\n",
       "['বিনোদন']                  64\n",
       "['মতামত']                   45\n",
       "['দূর পরবাস']               43\n",
       "['জীবনযাপন']                31\n",
       "['']                        18\n",
       "['অর্থনীতি']                16\n",
       "['আমরা']                    11\n",
       "['বিজ্ঞান ও প্রযুক্তি']      4\n",
       "['পাঁচমিশালি']               3\n",
       "['বিশেষ সংখ্যা']             2\n",
       "['শিক্ষা']                   1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = dataset[\"Document\"][:2000]\n",
    "x[0]\n",
    "\n",
    "cls = dataset[\"class\"][:1000]\n",
    "cls.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = [xs.replace('ইহাতে মন্তব্য প্রদান বন্ধ রয়েছে                                |    ', '') for xs in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl2 = [xs.replace('\\xa0', '').replace('\\u200c', '') for xs in cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl3 = [xs.replace('?', u'।').replace('!', u'।').replace(';', u'।') for xs in cl]\n",
    "\n",
    "\n",
    "\n",
    "# for x in cl3:\n",
    "#     print(x,\"\\n\\n\\n\")\n",
    "\n",
    "sentTok =[]\n",
    "wordTok = []\n",
    "for doc in cl3:\n",
    "    sentTok.append(doc.split('।'))\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pnList = ['.', ',', ':', '(', ')', '[', ']', '{', '}', '`', '*', '&', '‘', '’', '–']\n",
    "cl4 = [xs.replace(u'।',\" \") for xs in cl3]\n",
    "\n",
    "cl5=[[] for x in range(len(cl4))]\n",
    "for i in range(len(cl4)):\n",
    "#     cl5[i] = cl4[i].replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").replace(\"+\",\"\").replace(\")\",\"\")\n",
    "    cl5[i] = re.sub(\"[০১২৩৪৫৬৭৮৯]\",'',cl4[i])\n",
    "    cl5[i] = re.sub(\"[,./<>)(}{_*&^%$#@~`]\",'',cl5[i])\n",
    "\n",
    "for doc in cl5:\n",
    "    wordTok.append(doc.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['স্নাতক', 'পাস', 'আবুল', 'হাশেম', 'ও', 'তাঁর', 'ভাই', 'উচ্চমাধ্যমিক', 'পাস', 'সুজন', 'হোসেন', 'একসময়', 'ঢাকায়', 'গার্মেন্টসে', 'চাকরি', 'করতেন', 'তাঁদের', 'আরেক', 'ভাই', 'অষ্টম', 'শ্রেণি', 'পাস', 'ফরহাদ', 'হোসেন', 'আগে', 'নোয়াখালীতে', 'গ্রামের', 'বাড়িতে', 'কৃষিকাজ', 'করতেন', 'এখন', 'তিন', 'ভাইই', 'পরিবার', 'নিয়ে', 'ঢাকার', 'উত্তরার', 'বহুতল', 'ভবনে', 'বাস', 'করেন', 'ভাড়া', 'দেন', 'হাজার', 'টাকা', 'গত', 'শনিবার', 'তিন', 'ভাইয়ের', 'দুজন', 'সুজন', 'ও', 'ফরহাদকে', 'হাজার', 'পিছ', 'ইয়াবাসহ', 'উত্তরার', 'সেই', 'ফ্ল্যাট', 'থেকে', 'গ্রেপ্তার', 'করেছে', 'ঢাকা', 'মহানগর', 'গোয়েন্দা', 'পুলিশ', 'ডিবি', 'এই', 'ইয়াবার', 'আনুমানিক', 'মূল্য', 'লাখ', 'টাকা', 'আর', 'তিন', 'ভাইয়ের', 'ফ্ল্যাট', 'থেকে', 'জব্দ', 'করা', 'হয়েছে', 'আরও', 'নগদ', 'লাখ', 'হাজার', 'টাকা', 'রোববার', 'সুজন', 'ও', 'ফরহাদকে', 'ঢাকার', 'আদালতে', 'হাজির', 'করে', 'ডিবি', 'এক', 'প্রতিবেদন', 'দিয়ে', 'বলছে', 'হাশেম', 'সুজন', 'ও', 'ফরহাদ', 'তিন', 'ভাই', 'কয়েক', 'বছর', 'ধরে', 'কক্সবাজার', 'থেকে', 'ইয়াবার', 'চালান', 'ঢাকায়', 'এনে', 'জমজমাট', 'ব্যবসা', 'করছেন', 'তাঁদের', 'সঙ্গে', 'কারা', 'জড়িত', 'তা', 'খুঁজে', 'বের', 'করতে', 'দুই', 'ভাইকে', 'তিন', 'দিন', 'জিজ্ঞাসাবাদের', 'অনুমতি', 'দিয়েছেন', 'আদালত', 'তিনজনের', 'বাবা', 'ছিলেন', 'হকারমাদক', 'ব্যবসার', 'অভিযোগ', 'থাকা', 'এই', 'তিনজনের', 'বাবার', 'নাম', 'বিল্লাল', 'হোসেন', 'তাঁর', 'গ্রামের', 'বাড়ি', 'নোয়াখালী', 'অনেক', 'আগে', 'থেকে', 'তিনি', 'ঢাকায়', 'থাকেন', 'রাজধানীর', 'বিমানবন্দর', 'এলাকায়', 'ফুটপাতে', 'হকারি', 'করতেন', 'পরিবার', 'নিয়ে', 'থাকতেন', 'আশকোনাতে', 'বিল্লাল', 'হোসেন', 'এখন', 'এই', 'তিন', 'ছেলের', 'সঙ্গে', 'উত্তরার', 'ছেলেদের', 'ফ্ল্যাটে', 'থাকেন', 'হাজার', 'পিছ', 'ইয়াবাসহ', 'যখন', 'তাঁর', 'দুই', 'ছেলেকে', 'গ্রেপ্তার', 'করা', 'হয়', 'তখন', 'বিল্লাল', 'হোসেন', 'ওই', 'বাসাতেই', 'ছিলেন', 'দুই', 'ভাইকে', 'গ্রেপ্তার', 'অভিযানে', 'অংশ', 'নেওয়া', 'ঢাকা', 'মহানগর', 'গোয়েন্দা', 'পুলিশের', 'সহকারী', 'কমিশনার', 'এসি', 'মো', 'মাহবুবুল', 'আলম', 'প্রথম', 'আলোকে', 'বলেন', 'বিল্লাল', 'হোসেনের', 'ছেলে', 'সুজন', 'এর', 'আগেও', 'মাদকসহ', 'গ্রেপ্তার', 'হয়েছিল', 'এই', 'তিন', 'ভাই', 'বেশ', 'কয়েক', 'বছর', 'ধরে', 'মাদক', 'ব্যবসা', 'করে', 'আসছেন', 'তিন', 'ভাইয়ের', 'বাসায়', 'ইয়াবা', 'ব্যবসাপুলিশ', 'বলছে', 'এই', 'তিন', 'ভাইয়ের', 'মধ্যে', 'নেতা', 'হলেন', 'আবুল', 'হাশেম', 'কক্সবাজারের', 'বড়', 'বড়', 'মাদক', 'ব্যবসায়ীর', 'সঙ্গে', 'যোগাযোগ', 'রেখে', 'ইয়াবার', 'চালান', 'আনেন', 'ঢাকায়', 'এ', 'কাজে', 'তাঁরা', 'নোয়াখালীকে', 'ট্রানজিট', 'হিসেবে', 'ব্যবহার', 'করেন', 'আর', 'তাঁর', 'ভাই', 'সুজন', 'ও', 'ফরহাদ', 'ঢাকার', 'পাইকারি', 'ও', 'খুচরা', 'ব্যবসায়ীদের', 'কাছে', 'ইয়াবা', 'বিক্রি', 'করেন', 'মামলার', 'তদন্ত', 'কর্মকর্তা', 'ডিবির', 'পরিদর্শক', 'মুক্তার', 'হোসেন', 'প্রথম', 'আলোকে', 'বলেন', 'উত্তরার', 'ওই', 'ফ্ল্যাটে', 'মাদক', 'রাখতেন', 'তিন', 'ভাই', 'তাঁদের', 'ফ্ল্যাটে', 'মাদক', 'ব্যবসায়ীদের', 'আনাগোনা', 'ছিল', 'গ্রেপ্তার', 'অভিযানে', 'অংশ', 'নেওয়া', 'একাধিক', 'ডিবি', 'কর্মকর্তা', 'জানান', 'শনিবার', 'নোয়াখালী', 'থেকে', 'ইয়াবার', 'চালান', 'বাসে', 'করে', 'আনেন', 'সুজন', 'ও', 'ফরহাদ', 'তবে', 'তাঁদের', 'ভাই', 'আবুল', 'হাশেম', 'নোয়াখালীতেই', 'থেকে', 'যান', 'ভাইদের', 'গ্রেপ্তার', 'হওয়ার', 'খবর', 'শোনার', 'পর', 'হাশেম', 'গা–ঢাকা', 'দিয়েছেন', 'ডিবির', 'সহকারী', 'কমিশনার', 'মাহবুবুল', 'আলম', 'প্রথম', 'আলোকে', 'বলেন', 'ভাইদের', 'গ্রেপ্তার', 'খবর', 'শোনার', 'পর', 'থেকে', 'হাশেমের', 'মোবাইল', 'ফোন', 'বন্ধ', 'পাওয়া', 'যাচ্ছে', 'তবে', 'হাশেমকে', 'যেকোনো', 'সময়', 'গ্রেপ্তার', 'করা', 'হবে', 'মাহবুবুল', 'আলম', 'আরও', 'বলেন', 'সুজন', 'ও', 'ফরহাদদের', 'জিজ্ঞাসা', 'করে', 'জানতে', 'পেরেছেন', 'মাদক', 'ব্যবসা', 'করে', 'অনেক', 'টাকা', 'বানিয়েছেন', 'এই', 'ডিবি', 'কর্মকর্তা', 'মনে', 'করছেন', 'হাশেমদের', 'পুরো', 'পরিবার', 'মাদক', 'ব্যবসার', 'সঙ্গে', 'জড়িত']\n"
     ]
    }
   ],
   "source": [
    "print(wordTok[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "#remove stopword\n",
    "stRem = [[] for i in range(len(wordTok))]\n",
    "\n",
    "def remStWords(punctuF):\n",
    "    stWord = open(\"./stopwords-bn.txt\", encoding='utf8')\n",
    "    stWordFile = [i.rstrip() for i in stWord]\n",
    "    filtStWord = [w for w in punctuF if w not in stWordFile]\n",
    "\n",
    "    return filtStWord\n",
    "for i in range(len(wordTok)):\n",
    "    stRem[i] = remStWords(wordTok[i])\n",
    "    \n",
    "\n",
    "print(len(stRem[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "f = open(\"./allword.txt\", encoding=\"utf-8\")\n",
    "wordList = [i.rstrip() for i in f]\n",
    "\n",
    "\n",
    "def suffixSt(word):\n",
    "    # suffix dictionary\n",
    "    suffix = {\n",
    "        1: [u\"ি\", u\"ই\", u\"আ\", u\"ো\", u\"া\", u\"ঈ\", u\"ই\", u\"অ\", u\"ও\", u\"এ\",\n",
    "            u\"র\", u\"য়\", u\"ব\", u\"স\", u\" ঊ\", u\"ু\", u\"ো\", u\"ে\", u\"ী\", u\"য়\"],\n",
    "        \n",
    "        2: [u\"য়ে\", u\"না\", u\"লি\", u\"নি\", u\"না\", u\"য়া\", u\"ড়ি\", u\"িস\", u\"ান\", u\"ের\", u\"ার\", u\"ান\", u\"কু\"\n",
    "            u\"বি\",\n",
    "            u\"তে\", u\"রা\", u\"বে\", u\"মি\",\n",
    "            u\"সি\", u\" আল\", u\" উক\", u\"ড়ে\", u\"কে\", u\"টা\", u\"টি\", u\"টু\", u\"কা\", u\" রি\", u\"তি\", \" তা\", \"িল\", \"িক\", \"েক\",\n",
    "            \"েত\", \"েই\", \"আও\", \"আন\", \" আত\", \" আই\", \" অন\", \" অত\", \" অক\", \"ড়ে\", \"ড়ি\", \"ড়া\", \"সে\", \"সা\", \"লা\", \"মি\",\n",
    "            \"ভর\", \"নি\", \"তি\", \"তা\", \"টে\", \"টা\", \"চে\", \"কে\", \"কা\", \"এল\", \"উড়\", \"উক\", \"আল\", \"আর\", \"আম\", \"আত\", \"আচ\", \"আই\",\n",
    "            \"অড়\", \"অল\", \"তো\",\n",
    "            \"েছ\", \"েল\", \"ছে\",\"েও\"],\n",
    "        3: [\"ওয়া\", \" উরি\", \"উয়া\", \"উনি\", \"উকা\", u\"দের\", \"য়ের\",  \"কার\", \"েরা\",\n",
    "            \"িলা\", \"িনি\", \"তেই\", \"টাই\", \"ইয়ে\", \" ইয়া\", \" ইলে\", \" আরী\", \" আরি\", u\"আনো\", \" আনি\", \" আকু\", \" আইত\",\n",
    "            \" অন্ত\", \" অনি\", \" অনা\", \" অতি\", \" অতা\", \" োয়া\", \"ুয়া\", \"ুন্তি\", \"ুনি\", \"ুকা\", \"ুকা\", \"সিয়\", \"মন্ত\", \"ভরা\",\n",
    "            \"য়োন\", \"খান\", \"গুল\", \"পনা\", \"তুত\", \"উয়া\", \"উলি\", \"উরে\", \"ইয়া\", \"আলো\", \"আলি\", \"আলা\", \"আরু\", \"আরি\", \"আমো\",\n",
    "            \"আমি\",'তেন'\n",
    "            \"আনো\", \"আনি\", \"আচি\", \"আইত\",\n",
    "            \"চ্ছ\", \"েছে\", \"েলো\", \"ওয়া\", \"মান\"],\n",
    "        4: [u\"েদের\", \" অন্তি\", \"টিয়া\", \"কিয়া\", \" উন্তি\", u\"য়েরা\", u\"ভাবে\", \"খানা\", \"খানি\", \"গুলো\", \"গুলি\", \"পারা\",\n",
    "            \"পানো\",\n",
    "            \"ইয়াল\", \"য়েছে\", \"চ্ছি\", \"চ্ছে\", \"েছেন\"],\n",
    "        5: [u\"দেরকে\", \"চ্ছিল\", \"চ্ছিস\", u\"য়েদের\", \"ওয়ালা\", \"উড়িয়া\"],\n",
    "        6: [u\"েদেরকে\", \"চ্ছিলেন\"],\n",
    "        7: [u\"য়েদেরকে\"]\n",
    "    }\n",
    "    for w in 7, 6, 5, 4, 3, 2, 1:\n",
    "        if len(word) > w + 1:\n",
    "            for s in suffix[w]:\n",
    "                if word.endswith(s):\n",
    "                    return word[:-w]\n",
    "    return word\n",
    "\n",
    "def suffixRem(word):\n",
    "    i=0\n",
    "    if word in wordList:\n",
    "        print(word,\" l\")\n",
    "        i = wordList.index(word)\n",
    "        stW = suffixSt(wordList[i])\n",
    "        if  stW in wordList  :\n",
    "            if word in wordList  :\n",
    "                return word\n",
    "                \n",
    "            else:\n",
    "                print(stW,\" tt\")\n",
    "                return stW\n",
    "                \n",
    "        else:\n",
    "            return word\n",
    "    else:\n",
    "        W = word\n",
    "\n",
    "        return W\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13323\n"
     ]
    }
   ],
   "source": [
    "def makeList(sent):\n",
    "    li=[]\n",
    "    for w in sent:\n",
    "        li.append(suffixRem(w))\n",
    "    return li\n",
    "\n",
    "print(makeList(stRem))\n",
    "print(stRem)\n",
    "print(len(stRem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "গার্মেন্টস\n"
     ]
    }
   ],
   "source": [
    "print('গার্মেন্টস')\n",
    "if 'গার্মেন্টস'in wordList:\n",
    "    print(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(stRem))\n",
    "allWord = []\n",
    "for i in range(len(stRem)):\n",
    "    allWord+=stRem[i]\n",
    "    \n",
    "##Term frequency    \n",
    "from collections import Counter\n",
    "counts = Counter(allWord)\n",
    "# print(counts['অনেক'])\n",
    "tfDict = {}\n",
    "for sent in stRem:\n",
    "    for word in sent:\n",
    "        tfDict[word] = counts[word]/len(sent)\n",
    "#print(token)\n",
    "print(counts)\n",
    "#print(tfDict)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13323\n",
      "177153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#number of documents\n",
    "n = len(stRem)\n",
    "print(n)\n",
    "\n",
    "#Number of terms\n",
    "terms = list(tfDict.keys())\n",
    "print(len(terms))\n",
    "\n",
    "#Calculate Inverse Documentt frequency\n",
    "import math\n",
    "idfDict = {}\n",
    "n = 0\n",
    "for term in terms:\n",
    "    n+=1\n",
    "    print(n)\n",
    "    count = 0\n",
    "    for doc in stRem:\n",
    "        if term in doc:\n",
    "            count+=1\n",
    "    idfDict[term] = math.log(n/(count+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate tfIdf\n",
    "tfIdf = {}\n",
    "\n",
    "for term in terms:\n",
    "    tfIdf[term] = round(tfDict[term] * idfDict[term],4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8475"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfIdfSorted = {k:v for k,v in sorted(tfIdf.items(), key = lambda item: item[1], reverse = True)}\n",
    "\n",
    "len(tfIdfSorted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
